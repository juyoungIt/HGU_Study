# -*- coding: utf-8 -*-
"""3_imagedata_preprocessing_forstudent.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IDcYQpueeqPN43efgFG7HEklZE1SwpxB

# 1️⃣ Simple image data preprocessing
## Load image data
"""

# For colab users
# from google.colab import drive
# drive.mount('/content/drive')

import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from PIL import Image 
from numpy import asarray

img = Image.open('./data/school.jpg')
  
# asarray() class is used to convert PIL images into NumPy arrays 
numpydata = asarray(img) 

#  shape 
print(numpydata.shape) 
print(type(numpydata))
plt.imshow(numpydata)

"""## Understanding RGB"""

# plt.figure(figsize=(20,5))

# print('shape:', numpydata.shape) 
# print('type:', type(numpydata))

# plt.subplot(141)
# plt.imshow(numpydata[300:600, 300:600, :])
# plt.axis("off")

# plt.subplot(142)
# plt.imshow(_______)
# plt.axis("off")

# plt.subplot(143)
# plt.imshow(_______)
# plt.axis("off")

# plt.subplot(144)
# plt.imshow(_______)
# plt.axis("off")

"""## Change color & Resize & Rotate"""

img = _______
  
# asarray() class is used to convert PIL images into NumPy arrays 
numpydata = asarray(img) 
  
# <class 'numpy.ndarray'> 
print(type(numpydata)) 
  
#  shape 
print(numpydata.shape) 
plt.imshow(numpydata, cmap='gray')

print(img.size)
img2 = _______
print(img2.size)
plt.imshow(img2, cmap='gray')

img3 = _______
plt.imshow(img3, cmap='gray')

"""## Reshape & Scale"""

a = np.arange(6)

print('Before:', a.shape)
"""WRITE THE CODE"""
print('After:', a.shape)

from sklearn.preprocessing import StandardScaler

data = [[0, 0], [0, 0], [1, 1], [1, 1]]
"""WRITE THE CODE"""

"""## Histogram equalization"""

def histeq(im, nbr_bins=256):
    """  Histogram equalization of a grayscale image. """

    # get image histogram
    imhist, bins = histogram(im.flatten(), nbr_bins, density=True)
    cdf = imhist.cumsum() # cumulative distribution function
    cdf = 255 * cdf / cdf[-1] # normalize

    # use linear interpolation of cdf to find new pixel values
    im2 = interp(im.flatten(),bins[:-1],cdf)

    return im2.reshape(im.shape), cdf

from PIL import Image
from numpy import *

im = array(Image.open('./data/girl.jpg').convert('L'))
plt.imshow(im, cmap='gray')

im2,cdf = histeq(im)

print(type(im2))
plt.imshow(im2, cmap='gray')

"""## Blurring"""

from PIL import Image
from numpy import *
from scipy.ndimage import filters

im = array(Image.open('./data/girl.jpg').convert('L'))
im2 = filters.gaussian_filter(im, 3)

plt.imshow(im2, cmap='gray')

"""--------

# 2️⃣ MNIST tutorial (Exercise1~Exercise11)
## Exercise(1) - Processing MNIST data
"""

import pickle
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

print('... loading data')

# Read the MNIST data
with open('data/mnist.pkl', 'rb') as f:
    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')

train_x, train_y = train_set
test_x, test_y = test_set

'''
WRIT THE CODE
'''
# Create dataframe
train_x = pd.DataFrame(train_x)
train_y = pd.DataFrame(train_y, columns=['label'])
test_x = pd.DataFrame(test_x)
test_y = pd.DataFrame(test_y, columns=['label'])

# Concatenate x and y
train_data = pd.concat([train_x, train_y], axis=1)
test_data = pd.concat([train_x, train_y], axis=1)

"""+ ___DataFrame.values:___ Return a Numpy representation of the DataFrame."""

print(train_data.shape, test_data.shape)
train_data.head()

subset_images_X = train_data.iloc[:10, :-1]
subset_images_Y = train_data.iloc[:10, -1]

for i, row in subset_images_X.iterrows():
    ax = plt.subplot(2, 5, i+1)
    pixels = row.values.reshape((28, 28))
    plt.imshow(pixels, cmap='gray')
    plt.title('label: {}'.format(subset_images_Y[i]))
    
    plt.xticks([]) # erase the ticks
    plt.yticks([])

print(train_data.shape, test_data.shape)

"""## Exercise(2) - Template matching

### ✔️STEP1. Make binary data
"""

## binary train data
li = []
li = np.array([np.array([1 if pix!=0 else 0 for pix in train_x.iloc[i, :]]) for i in range(len(train_x))])
li.shape

## plot the image
image = li[10000, :].reshape((28, 28))
print(image)
plt.imshow(image, cmap='gray')

## make dataframe
bin_train_x = pd.DataFrame(li)
bin_train = pd.concat([bin_train_x, train_y], axis=1)
print('binary train data shape: ', bin_train.shape)

## binary test data
li = []
li = np.array([np.array([1 if pix!=0 else 0 for pix in test_x.iloc[i, :]]) for i in range(len(test_x))])
li.shape

bin_test_x = pd.DataFrame(li)
bin_test = pd.concat([bin_test_x, test_y], axis=1)
print('binary train data shape: ', bin_test.shape)

## check the original data
orig_img = train_x.iloc[10000, :].values.reshape((28, 28))
plt.imshow(orig_img, cmap='gray')
print(orig_img)

"""### ✔️STEP2.  Choose lable_0, label_1 data"""

## Data with label 0 and data with label 1
train_0_data = bin_train[train_data['label']==0]
train_1_data = bin_train[train_data['label']==1]

test_0_data = bin_test[test_y['label']==0]
test_1_data = bin_test[test_y['label']==1]
test_01_data = pd.concat([test_0_data, test_1_data])

print("train binary data shape: ", train_01_data.shape)
print("test binary data shape: ", test_01_data.shape)

"""### ✔️STEP3.  Template matching"""

def plot_images(img1, img2, img3, img4, img5):
    plt.figure(figsize=[30, 10])
    plt.subplot(151)
    plt.imshow(img1.values.reshape((28,28)), cmap='gray')
    plt.title('label-0 train data', fontsize=20)

    plt.subplot(152)
    plt.imshow(img2.values.reshape((28,28)), cmap='gray')
    plt.title('label-1 train data', fontsize=20)
    
    plt.subplot(153)
    plt.imshow(img5[:-1].values.reshape((28,28)), cmap='gray')
    plt.title('test data', fontsize=20)
    
    plt.subplot(154)
    plt.imshow(img3.reshape((28,28)), cmap='gray')
    plt.title('1st test data & label-0 train data', fontsize=20)

    plt.subplot(155)
    plt.imshow(img4.reshape((28,28)), cmap='gray')
    plt.title('1st test data & label-1 train data', fontsize=20)
    plt.show()

import numpy as np

# 1. Choose two representative images, randomly
tot_acc = 0
idx = 1000

subset_train_zero = train_0_data.iloc[idx, :-1]
subset_train_one = train_1_data.iloc[idx, :-1]

# 2. For each test data, apply template matching twice to label it with a label that has more than 1.
prediction = []

for i, (idx, test_data) in enumerate(test_01_data.iterrows()):
    tmp_match_zero = np.array([subset_train_zero.values & test_data[:-1]])
    tmp_match_one = np.array([subset_train_one.values & test_data[:-1]])

    prediction.append(0 if len(tmp_match_zero[tmp_match_zero==1]) > len(tmp_match_one[tmp_match_one==1]) else 1)
    
    if i==0:
        plot_images(subset_train_zero, subset_train_one, tmp_match_zero, tmp_match_one, test_data)
        
        print('Num of 1s after AND with 0-label train data: ', len(tmp_match_zero[tmp_match_zero==1]))
        print('Num of 1s after AND with 1-label train data: ', len(tmp_match_one[tmp_match_one==1]))
        print(prediction[0], '-lable yields larger number, thus ', (i+1), 'th test image is classfied as ', prediction[0], ' digit.')

"""### ❗️Expected Result
![image.png](attachment:image.png)
Num of 1s after AND with 0-label train data:  94  
Num of 1s after AND with 1-label train data:  47  
0 -lable yields larger number, thus  1 th test image is classfied as  0  digit.
"""

# 3. Compute the total accuracy
hits = (prediction == test_01_data.iloc[:, -1].values)
acc = sum(hits)/len(prediction)

print('\n Total accuracy on Test data is {:.4f}'.format(acc))

"""-----
## Exercise(3) - Standardize MNIST data [optional]
"""

# Not used in the later analysis
from sklearn.preprocessing import StandardScaler

## standard scaling
train_x = train_data.iloc[:, :-1]
train_y = train_data.iloc[:, -1]

"""
WRITE THE CODE
"""

"""## Exercise(4) - Covariance, Eigen vector / value"""

## dimension reduction scratch
train_x = train_data.iloc[:, :-1]
train_y = train_data.iloc[:, -1]

# calcualte covariance matrix, eigen values and eigen vectors
cov_matrix = np.cov(train_x.T)
eig_val, eig_vec = np.linalg.eig(cov_matrix)

# each column in eig_vec represents each eigen vector
# we want row vectors, thus perform transpose operation.
eig_vec = eig_vec.T
print('20 eigen values of 784 eigen values: ', eig_val[:20])

print('val:', eig_val.shape)
print('vec:', eig_vec.shape)

"""### ❗️Expected Result

20 eigen values of 784 eigen values:  [40.43479516 29.37970424 26.78343901 20.59801996 18.09538535 15.85867349
 13.84664883 12.46504288 11.07377749 10.04212206  9.66264599  8.66117667
  7.98176524  7.85288477  7.39824929  7.14866705  6.72531409  6.62115541
  6.38252628  6.25569094] <br>
val: (784,)  
vec: (784, 784)

----
## Exercise(5) - Visualize eigen vectors
"""

import matplotlib.pyplot as plt
import numpy as np

## Choose 2 vectors
good_vecs = eig_vec[:2]

plt.figure(figsize=(10, 5))
for i, vec in enumerate(good_vecs):
    vec = vec.reshape((28, 28)).real
    vec = np.array(vec)

    ax = plt.subplot(2, 5, i+1)
    fig = plt.imshow(vec, alpha=0.4, cmap='seismic')

"""### ❗️Expected Result
![image.png](attachment:image.png)
"""

bad_vecs = eig_vec[10:12]

plt.figure(figsize=(10, 5))
for i, vec in enumerate(bad_vecs):
    vec = vec.reshape((28, 28)).real
    vec = np.array(vec)

    ax = plt.subplot(2, 5, i+1)
    fig = plt.imshow(vec, alpha=0.4, cmap='seismic')

"""### ❗️Expected Result
![image.png](attachment:image.png)

https://colah.github.io/posts/2014-10-Visualizing-MNIST/
"""

zero_data = train_data[train_y==0].iloc[:, :-1].to_numpy()
one_data = train_data[train_y==1].iloc[:, :-1].to_numpy()
two_data = train_data[train_y==2].iloc[:, :-1].to_numpy()

plt.figure(figsize=(10, 5))
for i in range(3):
    vec = eig_vec[0].reshape((28,28)).real
    vec = np.array(vec)

    ax = plt.subplot(1, 3, i+1)
    
    if i==0:
        plt.imshow(zero_data[0].reshape((28, 28)), cmap='gray_r')
    elif i==1:
        plt.imshow(one_data[0].reshape((28, 28)), cmap='gray_r')
    elif i==2:
        plt.imshow(two_data[0].reshape((28, 28)), cmap='gray_r')
        
    plt.imshow(vec, alpha=0.4, cmap='seismic')

"""----
## Exercise(6) - Project data to matrix
+ ___np.vstack()___ : Stack arrays in sequence vertically (row wise).
"""

## Check the original array
train_data.head(3)

projected_x = train_x.dot(eig_vec[0:2].T).to_numpy()

print("new data points' shape: ", train_x.shape, "X", eig_vec[0:2].T.shape, "=", projected_x.shape) 
print("projected_x.shape: ", projected_x.shape, " train_y.shape: ", train_y.shape)

new_coordinates = np.vstack((projected_x.T, train_y)).T

dataframe = pd.DataFrame(data=new_coordinates, columns=("1st_pca", "2nd_pca", "label"))
dataframe.head(3)

"""----
## Exercise(7) - Visualize 2-dim MNIST data
"""

plt.scatter(projected_x[: ,0], projected_x[:, 1], s=5, c=train_y, cmap='Spectral')
plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))

plt.title('Visualizing MNIST through PCA', fontsize=24);
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

"""----
## Exercise(9) - Clustering & Visualization
"""

avg_point = []
X = []
Y = []

## calculate each label's mean value 
for i in range(0, 10):
    mean_data = projected_x[train_data['label']==i].mean(axis=0)
    X.append(mean_data[0])
    Y.append(mean_data[1])

## plot
plt.scatter(X, Y, s=50, c=[0,1,2,3,4,5,6,7,8,9], cmap='Spectral')
plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))

plt.title('Average point of PCA', fontsize=24);
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

di = {'x':X, 'y':Y}
avg_point_df = pd.DataFrame(di, columns=['x', 'y'])

avg_point_df

from math import *

def euclidean_distance(x, y):   
    return np.sqrt(np.sum((x - y) ** 2))

test_x, test_y = test_set
test_x = pd.DataFrame(test_x)
test_y = pd.DataFrame(test_y, columns=['label'])

projected_test_x = test_x.dot(eig_vec[0:2].T).to_numpy()
principal_df = pd.DataFrame(data = projected_test_x, columns = ['PC 1', 'PC 2'])

## Test data's PCA : test_x, test_y
print('test data\'s shape:', test_x.shape, test_y.shape)

pred_idx = []
print(projected_test_x.shape, test_y.shape)
for i, (x, y) in enumerate(zip(projected_test_x, test_y.squeeze())):
    ## calculate uclidean distance
    distance = [euclidean_distance(x, [avg_x, avg_y]) for (avg_x, avg_y) in ZIP(X, Y)]
    pred = distance.index(min(distance))
    pred_idx.append(pred)

print('pred_idx length: ', len(pred_idx))

principal_df['pred_label'] = pred_label
principal_df['label'] = test_y

principal_df.head()

plt.figure(figsize=(15,5))


plt.subplot(1,2,1)
plt.scatter(principal_df['PC 1'], principal_df['PC 2'], 
            s= 5, c=principal_df['pred_label'], cmap='Spectral')
plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))
plt.title('Predicted label', fontsize=24);
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

plt.subplot(1,2,2)
plt.scatter(principal_df['PC 1'], principal_df['PC 2'], 
            s= 5, c=principal_df['label'], cmap='Spectral')
plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))
plt.title('True label', fontsize=24);
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

acc = len(principal_df[principal_df['label']==principal_df['pred_label']])/len(principal_df['label'])
print('\n Total accuracy on Test data is {:.4f}'.format(acc))
print('---------------------------------------------')
print(principal_df)

"""----
## Exercise(10) - Explained variance
"""

nor_val = [] # Normalized eigen values
explained_variances = [] # explained_variances: accumulated eigen value's proportion
sums = np.sum(eig_val)

for i, v in enumerate(eig_val):
    nor_val.append(v/sums)
    explained_variances.append(sum(nor_val))

dic = {'eig_val': eig_val, 'nor_val': nor_val, 'explained_variance': explained_variance}
ev = pd.DataFrame(dic)
print(ev.head())

plt.figure(figsize=(10,3))
plt.subplot(1, 3, 1); plt.plot(eig_val); plt.title("eig_val")
plt.subplot(1, 3, 2); plt.plot(nor_val); plt.title("nor_val")
plt.subplot(1, 3, 3); plt.plot(explained_variances); plt.title("explained_variances")

# Find the dimension when the 'explained variance ratio' is 95%

expvar_threshold = 0.95
for i, v in enumerate(explained_variances):
    if v >= expvar_threshold:
        print('#choson PCs : ', i+1)
        break

print('784 dim(pixel): {:.4f}% is explained in 784-dim.'.format(explained_variances[784-1]*100))
print('329 dim(pixel): {:.4f}% is explained in 329-dim.'.format(explained_variances[329-1]*100))
print('2 dim(pixel): {:.4f}% is explained in 2-dim.'.format(explained_variances[2-1]*100))

"""### * PCA using library
+ ___class sklearn.decomposition.PCA___(__n_components=None,__ *, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)
"""

from sklearn.decomposition import PCA
eig_dim = [2, 5, 10]

for d in eig_dim:
    eig_train_data = PCA(n_components = d).fit_transform(train_x)
    print(f"dim: {d}, data\'s shape: {eig_train_data.shape}")

"""----
## Exercise(11) - Image reconstruction
"""

plt.figure(figsize=(10, 5));

# Original image
plt.subplot(1, 3, 1);
plt.imshow(train_x.iloc[100, :].values.reshape(28, 28), cmap='gray')
plt.title('Original Image', fontsize=20)

# Image reconstruction from PCA data
pca = PCA(n_components = 200);
approximation = pca.inverse_transform(eig_train_data)
plt.subplot(1, 3, 2)
plt.imshow(approximation[100, :].reshape(28, 28), cmap='gray')
plt.title('N = 200', fontsize=20)

pca = PCA(n_components = 100); eig_train_data = pca.fit_transform(train_x)
approximation = pca.inverse_transform(eig_train_data)
plt.subplot(1, 3, 3)
plt.imshow(approximation[100, :].reshape(28, 28), cmap='gray')
plt.title('N = 100', fontsize=20)