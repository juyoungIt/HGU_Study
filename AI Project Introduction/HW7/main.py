# -*- coding: utf-8 -*-
"""9_kMeans_forstudent.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LC7u6yNPzczoduHM-OcsBMBTU1vifV6n

# Week 10. Clustering 

## Exercise(1) Apply <font color='blue'>PCA algorithm
"""

import pickle
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

print('... loading data')
with open('data/mnist.pkl', 'rb') as f:
    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')

train_x, train_y = train_set
test_x, test_y = test_set

train_x = pd.DataFrame(train_x)
train_y = pd.DataFrame(train_y, columns=['label'])
test_x = pd.DataFrame(test_x)
test_y = pd.DataFrame(test_y, columns=['label'])

from sklearn.decomposition import PCA

mypca = PCA(n_components=2)
PCA_train_x = mypca.fit_transform(train_x)
PCA_test_x = mypca.transform(test_x)

print('PCA_train_x shape: ', PCA_train_x.shape)
print('PCA_test_x shape: ', PCA_test_x.shape)
print('PCA shape: ', PCA_train_x.shape, PCA_test_x.shape)

# Plot on the graph
plt.scatter(PCA_train_x[:, 0], PCA_train_x[:, 1], s=5, c=train_y['label'], cmap='Spectral')
plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))

plt.title('Visualizing MNIST through PCA', fontsize=24)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

"""### Let's use part of train_x data"""

sub_PCA_train_x = PCA_train_x[:1000, :]
print('sub_PCA_train_x.shape: ', sub_PCA_train_x.shape)

sub_PCA_test_x = PCA_test_x[:1000, :]
print('sub_PCA_test_x.shape: ', sub_PCA_test_x.shape)


"""## Exercise(2) - Hierarchical Clustering"""

from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram
import matplotlib.pyplot as plt

hier = AgglomerativeClustering(n_clusters=10, affinity='euclidean')
hier_clusters = hier.fit(sub_PCA_train_x)

# Plot
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.scatter(sub_PCA_train_x[:, 0], sub_PCA_train_x[:, 1], c=train_y['label'][:1000], cmap='Spectral', alpha=0.5)
plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))
plt.title("Clustering with ")
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

plt.subplot(1, 2, 2)
plt.scatter(sub_PCA_train_x[:, 0], sub_PCA_train_x[:, 1], c=hier_clusters.labels_[:1000], cmap='Spectral', alpha=0.5)
plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))
plt.title("Clustering with ")
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

# plt.show()

"""----
## Exercise(3) k-means scratch
### <font color='brown'>3-Step(1) Normalize data
"""

a = np.array([1, 3, 4])
A = np.tile(a, (3, 1))

"""two popular normalization methods
* min-max normalization
* mean-std normalization
"""


def apply_normalizer(dataset, offset, divisor):
    dataset_normalized = np.zeros(dataset.shape)
    N = dataset.shape[0]
    dataset_normalized = dataset - np.tile(offset, (N, 1))
    dataset_normalized = dataset_normalized / np.tile(divisor, (N, 1))

    return dataset_normalized


def normalize_minmax(dataset):
    minval = dataset.min(0)
    maxval = dataset.max(0)
    
    dataset_normalized = apply_normalizer(dataset, minval, maxval-minval)

    return dataset_normalized, minval, maxval-minval


def normalize_meanstd(dataset):
    meanval = dataset.mean(0)
    stdval = dataset.std(0)

    dataset_normalized = apply_normalizer(dataset, meanval, stdval)

    return dataset_normalized, meanval, stdval


normalized_PCA_train_x, off, div = normalize_minmax(sub_PCA_train_x)
print("Original data: ", sub_PCA_train_x[0], '\nNormalized data: ', normalized_PCA_train_x[0])
print("offset:", off, ";  divisor:", div, '\n')

normalized_PCA_train_x, off, div = normalize_meanstd(sub_PCA_train_x)
print("Original data: ", sub_PCA_train_x[0], '\nNormalized data: ', normalized_PCA_train_x[0])
print("offset:", off, ";  divisor:", div)

plt.scatter(normalized_PCA_train_x[:, 0], normalized_PCA_train_x[:, 1], cmap='Spectral', alpha=0.5)

""" <font color='blue'>

**Q1: In the normalization methods, what is the meaning of offset and divisor, respectively?** <br>

**Q2: After normalization, how does the data range change?** <br>

 <font color='black'>

Hint: Try np.mean(X_normalized, axis=0), np.std(X_normalized, axis=0), np.min(X_normalized, axis=0), np.max(X_normalized, axis=0), np.median(X_normalized, axis=0).
"""

# a distance function
def Euclidean_distance(vecA, vecB):
    return np.sqrt(np.sum((vecA - vecB) ** 2))

"""### <font color='brown'>3-Step(2). Initialize centroids"""

k = int(input("How many cluster do you want? "))
print(k)

import random


def init_centroids_random(dataset, k):
    """
    Arguments:
    dataset -- Numpy array of PCA applied data
    k -- the number of clusters
    """
    centroids = {}
    init_centroids = random.sample(range(0, len(dataset)), k)

    for i, c in enumerate(init_centroids):
        centroids[i] = dataset[c]
    return centroids


def init_centroids_index(dataset, k):
    """
    Arguments:
    dataset -- Numpy array of PCA applied data
    k -- the number of clusters
    """
    centroids = {}
    for i in range(k):  # first k instances become the initial centroids
        centroids[i] = dataset[i]
    return centroids


# initialize_centroids(centroids, sub_PCA_train_x)
centroids = init_centroids_random(sub_PCA_train_x, k)

# Change centroids value to dataframe.
cet_df = pd.DataFrame(centroids).transpose()
cet_df.columns = ['X', 'Y']
cet_df.head()

# Plot random centroids on the dataset

plt.figure()
plt.scatter(cet_df['X'], cet_df['Y'], color='black', marker='D')
plt.scatter(sub_PCA_train_x[:, 0], sub_PCA_train_x[:, 1], alpha=0.2)

"""### <font color='brown'>3-Step(3). (Re)assigning every datas to its _closest centroid_"""


def re_assign_data(dataset, centroids, cluster_memberships):
    """
    Arguments:
    dataset -- Numpy array of PCA applied data
    centroids -- A dictionary of centroids
    cluster_memberships -- A dictionary data which is clustered by key
                            (key: clustered group, value: values of that group)
    """
    # (Re)assigning every instance to its closest centroid
    for row in dataset:
        # YOUR CODE STARTS HERE
        # Calculate euclidean distance between each centroid and each data.
        dist_to_centroids = [Euclidean_distance(row, centroids[c]) for c in centroids]
        
        # Find the centroid with a minimum distance 
        membership = dist_to_centroids.index(min(dist_to_centroids))
        cluster_memberships[membership].append(row)

"""### <font color='brown'>3-Step(4). Recalculate average of each cluster and calculate SSE value"""


def re_calc_avg_sse(centroids, cluster_memberships):
    """
    Arguments:
    curr_sse -- Current SSE value which is initiated by 0
    cluster_memberships -- A dictionary data which is clustered by key(key: clustered group, value: values of that group)
    centroids -- A dictionary of centroids
    """
    curr_sse = 0
    # Re-calculate the average of each cluster and calculate SSE.
    for membership in cluster_memberships:
        centroids[membership] = np.average(cluster_memberships[membership], axis=0)

        for row in cluster_memberships[membership]:
            curr_sse += np.power(Euclidean_distance(row, centroids[membership]), 2)
    
    return curr_sse

"""###  <font color='brown'>3-Step(5). Iterate STEP3 and STEP4 until SSE is less than `tol` value 

"""

# k-Means algorithm
def kmeans(dataset, k, max_iter = 300, tol = 0.001):
    
    print('k is ', k)    

    centroids = init_centroids_index(dataset, k) # init_centroids_random(dataset, k)
    
    # 1. Initiate SSE which is key metric in k-means clustering (sse = sum of squared error) into 'np.inf'
    curr_sse = np.inf

    # 2. Clustering
    for i in range(max_iter):
        cluster_memberships = {}

        # Initiate cluster memberships
        for j in range(k):
            cluster_memberships[j] = []

        # (Re)Aassign datas to its closest centroids
        re_assign_data(dataset, centroids, cluster_memberships)

        # Re-calculate the average of each cluster and calculate SSE.
        prev_sse = curr_sse
        curr_sse = re_calc_avg_sse(centroids, cluster_memberships)

        # Plot center points
        plt.figure(i)
        c_df = pd.DataFrame(centroids).transpose()
        plt.scatter(c_df.loc[:, 0], c_df.loc[:, 1], color='black', marker='x')

        # Plot assigned data
        for key in cluster_memberships:
            plt.scatter(*zip(*cluster_memberships[key]), alpha=0.2)
            plt.title('k={} '.format(k) + ' SSE=' + str(curr_sse))

        plt.show()
        '''
        Stop iteration if "(prev_sse - curr_sse) / curr_sse" is less than 'tol' value. 
        It means you don't have to reassign datas since every data is already clustered.
        '''    
        print('iteration#{} | prev_sse= {:.4f};  curr_sse= {:.4f}'.format(i, prev_sse, curr_sse))
        # the solution part of Q5
        if (prev_sse - curr_sse) / curr_sse < tol:
            break

    return cluster_memberships, curr_sse

"""
 <font color='blue'>

**Q3: Before the iterations, how are the centroids defined?**<br>

**Q4: One metric to evaluate the clustering results is sum of squared error (SSE). Describe the meaning of SSE in terms of the relationship between data and centroids**<br>

**Q5: What is the terminal condition? Describe it with `tol` and `max_iter`.**"""

cluster_memberships, curr_sse = kmeans(sub_PCA_train_x, k)

"""----
### Step(6) Using sklearn library
- ___KMeans(n_clusters=)___ 
"""

from sklearn.cluster import KMeans

model = KMeans(n_clusters=k)
model.fit(PCA_train_x)

result = model.predict(PCA_test_x)

"""- Check ___crosstab___"""

import pandas as pd

df = pd.DataFrame({'labels': test_y['label'], 'result': result})
ct = pd.crosstab(df['labels'], df['result'])
print(ct)

plt.show()